
# ğŸ•µï¸â€â™‚ï¸ Credit Card Fraud Detection â€” DI Hackathon 2025

A data analysis & machine learning project developed for the **Developers Institute Hackathon (October 2025)**.  
The goal is to detect fraudulent credit-card transactions using data-driven techniques and machine-learning models.

---

## ğŸš€ Overview

Fraudulent transactions are **extremely rare** â€” less than 0.2 % of all transactions.  
This project demonstrates how to:

1. Explore and understand the data through **EDA (Exploratory Data Analysis)**  
2. Handle **imbalanced data** using **SMOTE**  
3. Train and evaluate several **classification models**:
   - Logistic Regression  
   - Random Forest  
   - XGBoost  
4. Tune decision thresholds to **maximize recall** (catch more frauds)  
5. Export model reports and feature-importance rankings for analysis

---

## ğŸ§° Tech Stack

| Category | Tools / Libraries |
|-----------|-------------------|
| Core | Python 3.10 + |
| Data Handling | Pandas, NumPy |
| Visualization | Matplotlib, Seaborn |
| ML & Evaluation | Scikit-learn, XGBoost, Imbalanced-Learn |
| Notebook & Reports | Jupyter Notebook / Google Colab |
| Environment | Virtualenv + pip (requirements.txt) |

---

## ğŸ“¦ Dataset

**Source :** [Kaggle â€“ Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)  
- 284 807 transactions (492 fraudulent)  
- 30 anonymized PCA features (V1â€“V28 + Time + Amount)  
- Target column `Class`: `0 = Legit`, `1 = Fraud`

Place the dataset file here:


fraud-detection-di-hackathon/data/creditcard.csv



---

## ğŸ§  Project Structure

```

fraud-detection-di-hackathon/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ creditcard.csv
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ fraud_analysis_di_hackathon.ipynb      â† Google Colab / Jupyter version
â”œâ”€â”€ fraud_analysis_di_hackathon.py             â† main script (executable)
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ model_summary.csv                      â† model metrics
â”‚   â”œâ”€â”€ feature_importance_rf.csv              â† Random Forest importance
â”‚   â””â”€â”€ feature_importance_xgb.csv             â† XGBoost importance
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

```

---

## âš™ï¸ Installation & Execution (Local)

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/Koss-Lab/fraud-detection-di-hackathon.git
cd fraud-detection-di-hackathon
```

### 2ï¸âƒ£ Create & Activate a Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate          # Mac/Linux
venv\Scripts\activate             # Windows
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

 ğŸ§© If youâ€™re on macOS and see an error with XGBoost (libomp):

```bash
brew install libomp
export DYLD_LIBRARY_PATH=/opt/homebrew/opt/libomp/lib:$DYLD_LIBRARY_PATH
```

### 4ï¸âƒ£ Run the Script

```bash
python3 fraud_analysis_di_hackathon.py
```

This will:

* Load and analyze the dataset
* Balance it with SMOTE
* Train Logistic Regression, Random Forest & XGBoost
* Save results to `/reports/`
* Display evaluation metrics and plots

---

## ğŸ’» Execution on Google Colab

If you prefer running it online:
ğŸ‘‰ [**Open in Google Colab**](https://colab.research.google.com/drive/1gtHC7pGkNpZDpFUbTtoWx6FrIkW9R5DY?usp=sharing)

Steps :

1. Upload the dataset (`creditcard.csv`) when prompted.
2. Run all cells sequentially.
3. The notebook will automatically download the `/reports` CSV files when finished.

---

## ğŸ“ˆ Results & Insights

| Model                       | Precision | Recall |   F1 |  AUC |
| :-------------------------- | --------: | -----: | ---: | ---: |
| Logistic Regression (tuned) |      0.72 |   0.85 | 0.78 | 0.97 |
| Random Forest               |      0.72 |   0.86 | 0.79 | 0.98 |
| XGBoost                     |      0.50 |   0.86 | 0.63 | 0.98 |

**Interpretation :**

* All models achieved high AUC (â‰ˆ 0.98) â†’ excellent discrimination between fraud / legit.
* Random Forest gave the best balance between precision and recall.
* Tuned Logistic Regression performed almost as well with lighter computation.

### Feature Importance

The most influential variables for detecting fraud were PCA features `V14`, `V17`, `V12`, `V10`, and `V4`.

---

## ğŸ§® Key Concepts

| Metric        | Meaning                                                  |
| ------------- | -------------------------------------------------------- |
| **Precision** | Among predicted frauds, how many are actually fraudulent |
| **Recall**    | Among all real frauds, how many were caught by the model |
| **F1 / F2**   | Balance between precision and recall (F2 â†’ favor recall) |
| **AUC (ROC)** | Modelâ€™s overall ability to separate fraud vs non-fraud   |

---

## ğŸ“Š Generated Reports

Once executed, youâ€™ll find in `/reports/` :

* `model_summary.csv` â†’ comparative metrics for each model
* `feature_importance_rf.csv` â†’ Random Forest top features
* `feature_importance_xgb.csv` â†’ XGBoost top features

Example :

```
reports/
â”œâ”€â”€ model_summary.csv
â”œâ”€â”€ feature_importance_rf.csv
â””â”€â”€ feature_importance_xgb.csv
```

---


## ğŸ‘¨â€ğŸ’» Author

**Ariel Kossmann**
Developers Institute â€” Tel Aviv ğŸ‡®ğŸ‡±

---

