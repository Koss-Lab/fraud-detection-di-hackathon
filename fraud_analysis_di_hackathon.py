# -*- coding: utf-8 -*-
"""fraud_analysis_di_hackathon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gtHC7pGkNpZDpFUbTtoWx6FrIkW9R5DY
"""

# === IMPORTS ===
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

sns.set(style="whitegrid")
plt.rcParams["figure.figsize"] = (10, 5)

# === SHOW ALL COLUMNS ===
pd.set_option('display.max_columns', None)

import os
import pandas as pd

# ‚úÖ Detect environment: Colab or Local
try:
    from google.colab import files
    COLAB = True
except ImportError:
    COLAB = False

if COLAB:
    print("Please upload the dataset manually (creditcard.csv)")
    uploaded = files.upload()
    file_name = list(uploaded.keys())[0]
else:
    # Local path (if dataset is already in your repo)
    file_name = os.path.join("data", "creditcard.csv")

df = pd.read_csv(file_name)
print(f"‚úÖ Loaded dataset from '{file_name}' successfully!")

# Basic info
df.info()
print("\nShape:", df.shape)

# Fraud vs Non-fraud
fraud_counts = df['Class'].value_counts()
sns.barplot(x=fraud_counts.index, y=fraud_counts.values)
plt.title("Class Distribution: Legit (0) vs Fraud (1)")
plt.xlabel("Class")
plt.ylabel("Count")
plt.show()

fraud_ratio = fraud_counts[1] / fraud_counts.sum() * 100
print(f"Fraud ratio: {fraud_ratio:.4f}%")

# üíµ Amount distribution
sns.histplot(df['Amount'], bins=100, kde=True)
plt.title("Distribution of Transaction Amount")
plt.xlabel("Amount (‚Ç¨)")
plt.show()

# ‚è∞ Time distribution
sns.histplot(df['Time'], bins=100, kde=True)
plt.title("Distribution of Time (seconds since first transaction)")
plt.xlabel("Time (s)")
plt.show()

# ‚ö° PCA Feature by Class
sns.kdeplot(data=df, x="V1", hue="Class", fill=True, common_norm=False)
plt.title("Distribution of V1 by Class")
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X = df.drop(columns=['Class'])
y = df['Class']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=42, stratify=y
)

# Scale only 'Time' and 'Amount' (others already PCA)
scaler = StandardScaler()
for col in ['Time', 'Amount']:
    X_train[col] = scaler.fit_transform(X_train[[col]])
    X_test[col]  = scaler.transform(X_test[[col]])

print("Train:", X_train.shape, "Test:", X_test.shape)
print("Fraud ratio train:", y_train.mean()*100, "%")

from imblearn.over_sampling import SMOTE

#
smote = SMOTE(random_state=42, sampling_strategy=0.2)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

print("After SMOTE:", y_train_res.value_counts())
sns.countplot(x=y_train_res)
plt.title("Balanced dataset after SMOTE")
plt.show()

# === MODEL EVALUATION UTILITIES ===
# This cell defines AND confirms the functions are loaded correctly üëá

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score,
    RocCurveDisplay, PrecisionRecallDisplay, fbeta_score
)

def evaluate_model(model, Xtr, ytr, Xte, yte, name=None, threshold=0.5):
    """
    Train a classification model and show key metrics + visualizations.
    - model: sklearn or XGBoost model with .fit() and .predict_proba()
    - threshold: probability cutoff for class 1 (fraud)
    """
    if name is None:
        name = model.__class__.__name__

    print(f"\nüöÄ Training {name}...")
    model.fit(Xtr, ytr)
    proba = model.predict_proba(Xte)[:, 1]
    y_pred = (proba >= threshold).astype(int)

    # === METRICS ===
    print(f"\n=== {name} (threshold={threshold:.2f}) ===")
    print(classification_report(yte, y_pred, digits=4))
    print("ROC-AUC:", round(roc_auc_score(yte, proba), 4))

    # === CONFUSION MATRIX ===
    cm = confusion_matrix(yte, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap="Blues")
    plt.title(f"Confusion Matrix ‚Äî {name}")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.show()

    # === ROC CURVE ===
    RocCurveDisplay.from_predictions(yte, proba)
    plt.title(f"ROC Curve ‚Äî {name}")
    plt.show()

    # === PRECISION-RECALL CURVE ===
    PrecisionRecallDisplay.from_predictions(yte, proba)
    plt.title(f"Precision-Recall ‚Äî {name}")
    plt.show()

    return proba


def find_best_threshold(y_true, proba, beta=2.0):
    """
    Find the best classification threshold maximizing the F-beta score.
    beta > 1 means we value Recall more than Precision.
    """
    ts = np.linspace(0.01, 0.99, 99)
    scores = [(t, fbeta_score(y_true, (proba>=t).astype(int), beta=beta)) for t in ts]
    best_t, best_s = max(scores, key=lambda x: x[1])
    print(f"‚úÖ Best threshold for F{beta}: {best_t:.2f} (score={best_s:.4f})")
    return best_t


# === Confirm everything is loaded correctly ===
print("‚úÖ Functions 'evaluate_model()' and 'find_best_threshold()' are ready to use!")

from sklearn.linear_model import LogisticRegression

# üß± Define baseline model
log_reg = LogisticRegression(max_iter=1000, n_jobs=-1)

# üöÄ Train & evaluate
proba_lr = evaluate_model(
    log_reg,
    X_train_res, y_train_res,
    X_test, y_test,
    name="Logistic Regression"
)

# üéØ Find the optimal threshold (F2-score ‚Üí Recall is more important)
t_star = find_best_threshold(y_test, proba_lr, beta=2.0)

# üîÅ Re-evaluate with tuned threshold
_ = evaluate_model(
    log_reg,
    X_train_res, y_train_res,
    X_test, y_test,
    name="Logistic Regression (tuned)",
    threshold=t_star
)

from sklearn.ensemble import RandomForestClassifier

# üå≤ Random Forest (optimized for speed)
rf = RandomForestClassifier(
    n_estimators=200,
    max_depth=12,
    min_samples_leaf=2,
    n_jobs=-1,
    random_state=42
)

proba_rf = evaluate_model(
    rf,
    X_train_res, y_train_res,
    X_test, y_test,
    name="Random Forest"
)

from xgboost import XGBClassifier

# ‚ö° XGBoost tuned for balanced performance
xgb = XGBClassifier(
    n_estimators=300,
    max_depth=4,
    learning_rate=0.05,
    subsample=0.9,
    colsample_bytree=0.9,
    reg_lambda=1.0,
    n_jobs=-1,
    random_state=42,
    eval_metric="logloss"
)

proba_xgb = evaluate_model(
    xgb,
    X_train_res, y_train_res,
    X_test, y_test,
    name="XGBoost"
)

# Simple summary table
from sklearn.metrics import precision_score, recall_score, f1_score

models = {
    "LogisticRegression": (log_reg, proba_lr),
    "RandomForest": (rf, proba_rf),
    "XGBoost": (xgb, proba_xgb),
}

summary = []
for name, (model, proba) in models.items():
    y_pred = (proba >= 0.5).astype(int)
    summary.append({
        "Model": name,
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1": f1_score(y_test, y_pred),
        "AUC": roc_auc_score(y_test, proba)
    })

pd.DataFrame(summary).sort_values("AUC", ascending=False)

# === FEATURE IMPORTANCE ===
import matplotlib.pyplot as plt
import pandas as pd

# Random Forest Feature Importance
importances = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)
importances.head(10).plot(kind='bar', color='teal')
plt.title("Top 10 Most Important Features ‚Äî Random Forest")
plt.ylabel("Importance Score")
plt.show()

# XGBoost Feature Importance
xgb_importances = pd.Series(xgb.feature_importances_, index=X_train.columns).sort_values(ascending=False)
xgb_importances.head(10).plot(kind='bar', color='orange')
plt.title("Top 10 Most Important Features ‚Äî XGBoost")
plt.ylabel("Importance Score")
plt.show()

# === FEATURE IMPORTANCE EXPORTS ===
import os

# ‚úÖ Fix path for both Colab and Local
reports_dir = os.path.join(os.getcwd(), "reports")
os.makedirs(reports_dir, exist_ok=True)

# === Save CSV reports ===
results_path = os.path.join(reports_dir, "model_summary.csv")
rf_path = os.path.join(reports_dir, "feature_importance_rf.csv")
xgb_path = os.path.join(reports_dir, "feature_importance_xgb.csv")

# build dataframe before saving
results_df = pd.DataFrame(summary).sort_values("AUC", ascending=False)

# export
results_df.to_csv(results_path, index=False)
importances.to_csv(rf_path, header=["importance"])
xgb_importances.to_csv(xgb_path, header=["importance"])

print(f"üìä Reports saved to: {os.path.abspath(reports_dir)}")

# === Detect if running in Google Colab ===
try:
    import google.colab
    IN_COLAB = True
except ImportError:
    IN_COLAB = False

# === If Colab: download files directly ===
if IN_COLAB:
    from google.colab import files
    print("‚¨áÔ∏è Downloading reports to your computer...")
    files.download(results_path)
    files.download(rf_path)
    files.download(xgb_path)
else:
    print("‚úÖ Running locally ‚Äî files exported to /reports folder.")